{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef052047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "347bdff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball_stemmer=SnowballStemmer('english')\n",
    "wordnet_lemmatizer=WordNetLemmatizer()\n",
    "from autocorrect import Speller\n",
    "spell=Speller(lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7fde323",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __int__(self):\n",
    "        pass\n",
    "\n",
    "    def spell_correct(self,text):       \n",
    "        spells = [spell(word) for word in (nltk.word_tokenize(text))]\n",
    "        return \" \".join(spells)\n",
    "\n",
    "    def to_lower(self,text):\n",
    "        return text.lower()\n",
    "\n",
    "    def remove_numbers(self,text):\n",
    "        output = ''.join(c for c in text if not c.isdigit())\n",
    "        return output\n",
    "\n",
    "    def remove_punct(self,text):\n",
    "        return ''.join(c for c in text if c not in punctuation)\n",
    "\n",
    "    def remove_Tags(self,text):\n",
    "        regex = re.compile(r'<[^>]+>')\n",
    "        return regex.sub('', text)\n",
    "\n",
    "    def sentence_tokenize(self,text):\n",
    "        sent_list = []\n",
    "        for w in nltk.sent_tokenize(text):\n",
    "            sent_list.append(w)\n",
    "        return sent_list\n",
    "\n",
    "    def word_tokenize(self,text):\n",
    "        return [w for sent in nltk.sent_tokenize(text) for w in nltk.word_tokenize(sent)]\n",
    "\n",
    "    def remove_stopwords(self,sentence):\n",
    "        stop_words = stopwords.words('english')\n",
    "        return ' '.join([w for w in nltk.word_tokenize(sentence) if not w in stop_words])\n",
    "\n",
    "    def stem(self,text):\n",
    "        stemmed_word = [snowball_stemmer.stem(word) for sent in nltk.sent_tokenize(text)for word in nltk.word_tokenize(sent)]\n",
    "        return \" \".join(stemmed_word)\n",
    "\n",
    "    def lemmatize(self,text):\n",
    "        lemmatized_word = [wordnet_lemmatizer.lemmatize(word)for sent in nltk.sent_tokenize(text)for word in nltk.word_tokenize(sent)]\n",
    "        return \" \".join(lemmatized_word)\n",
    "    def preprocess(self,text):\n",
    "        lower_text = self.to_lower(text)\n",
    "        sentence_tokens = self.sentence_tokenize(lower_text)\n",
    "        word_list = []\n",
    "        for sent in sentence_tokens:\n",
    "            clean_text = self.remove_Tags(sent)\n",
    "            print(\"\\nSentence after Removing Tags:\\n\",clean_text)\n",
    "            \n",
    "            clean_text = self.remove_punct(clean_text)\n",
    "            print(\"\\nSentence after Removing punctuations:\\n\",clean_text)\n",
    "        \n",
    "            clean_text = self.remove_numbers(clean_text)\n",
    "            print(\"\\nSentence after Removing numbers:\\n\",clean_text)\n",
    "            \n",
    "            clean_text = self.remove_stopwords(clean_text)\n",
    "            print(\"\\nSentence after Removing Stopwords:\\n\",clean_text)\n",
    "            \n",
    "            clean_text = self.lemmatize(clean_text)\n",
    "            print(\"\\nSentence after lemmatization:\\n\",clean_text)\n",
    "            \n",
    "            clean_text = self.spell_correct(clean_text)\n",
    "            print(\"\\nSentence after Spelling corrections:\\n\",clean_text)            \n",
    "           \n",
    "            word_tokens = self.word_tokenize(clean_text)\n",
    "            for word in word_tokens:\n",
    "                word_list.append(word)\n",
    "        return word_list\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5239a29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a filee, which is a part of a huuge 123 corpora which contains1 informasion about' natral, language procesing.\n",
      "\n",
      "Sentence after Removing Tags:\n",
      " this is a filee, which is a part of a huuge 123 corpora which contains1 informasion about' natral, language procesing.\n",
      "\n",
      "Sentence after Removing punctuations:\n",
      " this is a filee which is a part of a huuge 123 corpora which contains1 informasion about natral language procesing\n",
      "\n",
      "Sentence after Removing numbers:\n",
      " this is a filee which is a part of a huuge  corpora which contains informasion about natral language procesing\n",
      "\n",
      "Sentence after Removing Stopwords:\n",
      " filee part huuge corpora contains informasion natral language procesing\n",
      "\n",
      "Sentence after lemmatization:\n",
      " filee part huuge corpus contains informasion natral language procesing\n",
      "\n",
      "Sentence after Spelling corrections:\n",
      " file part huge corpus contains information natural language processing\n",
      "\n",
      "Final processed unique tokens in the file:\n",
      " ['file', 'part', 'huge', 'corpus', 'contains', 'information', 'natural', 'language', 'processing']\n"
     ]
    }
   ],
   "source": [
    "file = open(\"sample.txt\", \"r\")\n",
    "text = file.read()\n",
    "print(text)\n",
    "pr = Preprocess()\n",
    "word_list = pr.preprocess(text)\n",
    "print (\"\\nFinal processed unique tokens in the file:\\n\",word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ffb6b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44b267ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer()\n",
    "x=cv.fit_transform(word_list)\n",
    "x=x.toarray()\n",
    "vacabulary=sorted(cv.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a7fc85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contains', 'corpus', 'file', 'huge', 'information', 'language', 'natural', 'part', 'processing']\n",
      "[[0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(vacabulary)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c465ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
